{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "import random\n",
    "from sklearn.linear_model import Ridge\n",
    "from fancyimpute import SoftImpute, BiScaler\n",
    "from sklearn.linear_model import LinearRegression as reg\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "import itertools\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "import os, sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data=np.load('august2020-exercise1.npz')\n",
    "\n",
    "#print(data.files) # view the files in the npz\n",
    "\n",
    "#function to normalize the data\n",
    "def normalize(matrix):\n",
    "    normed=(matrix-matrix.mean(axis=0))/matrix.std(axis=0)\n",
    "    return normed\n",
    "\n",
    "######## train data ########\n",
    "X_lr_small_train=normalize(data['X_lr_small_train'])\n",
    "X_lr_big_train=normalize(data['X_lr_big_train'])\n",
    "X_hr_small_train=normalize(data['X_hr_small_train'])\n",
    "X_hr_big_train=normalize(data['X_hr_big_train'])\n",
    "\n",
    "y_lr_small_train=normalize(data['y_lr_small_train'])\n",
    "y_lr_big_train=normalize(data['y_lr_big_train'])\n",
    "y_hr_small_train=normalize(data['y_hr_small_train'])\n",
    "y_hr_big_train=normalize(data['y_hr_big_train'])\n",
    "######## test data#########\n",
    "X_lr_test=normalize(data['X_lr_test'])\n",
    "X_hr_test=normalize(data['X_hr_test'])\n",
    "\n",
    "y_lr_test=normalize(data['y_lr_test'])\n",
    "y_hr_test=normalize(data['y_hr_test'])\n",
    "\n",
    "def compRandomFunc(array,probability): # probability=0.7\n",
    "    n=array.shape[0]\n",
    "    p=array.shape[1]\n",
    "    array=array.astype(\"float\")\n",
    "    for i in range(n):    \n",
    "        for j in range(p):\n",
    "            r=random.uniform(0, 1)\n",
    "            if r < probability:\n",
    "                array[i,j]=np.NaN\n",
    "    return array\n",
    "\n",
    "def missingByFeature(array,p1,p2): #p1=0.875, p2=0.8\n",
    "    n=array.shape[0]\n",
    "    p=array.shape[1]\n",
    "    array=array.astype(\"float\")\n",
    "    featureList=np.array([])\n",
    "    for i in range(p):\n",
    "        r = random.uniform(0, 1)\n",
    "        if r<p1:\n",
    "            featureList=np.append(featureList,i)\n",
    "    for i in featureList:  # i is the counter for feature not sample size\n",
    "        i=int(i)\n",
    "        for j in range(n):\n",
    "            r2=random.uniform(0,1)\n",
    "            if r2<p2:\n",
    "                array[j,i]=np.NaN\n",
    "    return array\n",
    "\n",
    "def meanImputation(feature,data):\n",
    "    imputed=np.nanmean(data[:,feature])\n",
    "    return imputed\n",
    "\n",
    "# creating data with missing values\n",
    "import math\n",
    "probability=0.7\n",
    "p1=0.875\n",
    "p2=0.8\n",
    "# return the imputed matrix with mean imputation\n",
    "def imputetMissing(matrix):\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            if math.isnan(matrix[i,j]) is True:\n",
    "                matrix[i,j]=meanImputation(j,matrix)\n",
    "    return matrix\n",
    "\n",
    "def test(models, X_train,y_train,X_test,y_test, iterations = 100):\n",
    "    results = {}\n",
    "    for i in models:\n",
    "        r2_train = []\n",
    "        r2_test = []\n",
    "        for j in range(iterations):\n",
    "            r2_test.append(metrics.r2_score(y_test,\n",
    "                                            models[i].fit(X_train, \n",
    "                                                         y_train).predict(X_test)))\n",
    "\n",
    "            r2_train.append(metrics.r2_score(y_train, \n",
    "                                             models[i].fit(X_train, \n",
    "                                                          y_train).predict(X_train)))\n",
    "        results[i] = [np.mean(r2_train),np.mean(r2_test)]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def importantFeatures(best_model,X_train,y_train,BS_size):\n",
    "    importantFeatures=np.array([])\n",
    "    \n",
    "    for i in range(100):\n",
    "        #creatig bootstrap boots\n",
    "        x_bag=np.empty((0,X_train.shape[1]))\n",
    "        y_bag=np.array([])\n",
    "        for j in range(BS_size): \n",
    "            randIndex=np.random.randint(len(X_train), size=1)\n",
    "            x_sample=X_train[randIndex,:]\n",
    "            y_sample=y_train[randIndex]\n",
    "                \n",
    "            x_bag=np.concatenate((x_bag,x_sample))\n",
    "            y_bag=np.append(y_bag,y_sample)             \n",
    "        # test the estimator on bootstrap sample 100 times\n",
    "        #skippa grid search och kör dirr på bästa hyperparametrarna för att spara tid\n",
    "        bestEstimator=best_model.fit(x_bag,y_bag)\n",
    "        \n",
    "        rfeBest=RFE(bestEstimator)\n",
    "        X_rfe=rfeBest.fit_transform(x_bag,y_bag)\n",
    "        bestEstimator.fit(X_rfe,y_bag)\n",
    "        \n",
    "        importantFeatures=np.append(importantFeatures,np.where(rfeBest.ranking_==1))\n",
    "        \n",
    "    return importantFeatures\n",
    "\n",
    "def featureImportance(featureList):\n",
    "    unique_elements, counts_elements = np.unique(featureList, return_counts=True)\n",
    "    return unique_elements, counts_elements\n",
    "\n",
    "def mostCommonFeatures(occurrence):\n",
    "    mostCommonIndex=np.array([])\n",
    "    for i in range(len(occurrence)):\n",
    "        if occurrence[i]>=95:\n",
    "            mostCommonIndex=np.append(mostCommonIndex,[i])\n",
    "    mostCommonIndex=np.array(mostCommonIndex,dtype=np.int8)\n",
    "    mostCommonFeatures=featureIndex[mostCommonIndex]\n",
    "    return mostCommonFeatures\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low rate small data\n",
      "ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0.25,\n",
      "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "models = {'OLS': linear_model.LinearRegression(),\n",
    "         'Lasso': linear_model.Lasso(),\n",
    "         'Ridge': linear_model.Ridge(),}\n",
    "\n",
    "lasso_params = {'alpha':[0.02, 0.024, 0.025, 0.026, 0.03]}\n",
    "ridge_params = {'alpha':[200, 250, 300, 400, 500]}\n",
    "eNet_params={'alpha': [0, 0.5, 0.1, 0.01, 0.001],\n",
    "             'l1_ratio': [0, 0.25, 0.5, 0.75, 1]}\n",
    "\n",
    "models1 = {\n",
    "           'Lasso': GridSearchCV(linear_model.Lasso(), \n",
    "                               param_grid=lasso_params).fit(X_lr_small_train,y_lr_small_train).best_estimator_,\n",
    "           'Ridge': GridSearchCV(linear_model.Ridge(), \n",
    "                               param_grid=ridge_params).fit(X_lr_small_train,y_lr_small_train).best_estimator_,\n",
    "          'Elastic net':GridSearchCV(linear_model.ElasticNet(), \n",
    "                               param_grid=eNet_params).fit(X_lr_small_train,y_lr_small_train).best_estimator_,\n",
    "         'OLS': linear_model.LinearRegression()}\n",
    "\n",
    "print('low rate small data')\n",
    "test(models1, X_lr_small_train,y_lr_small_train,X_lr_test,y_lr_test)\n",
    "\n",
    "print(GridSearchCV(linear_model.ElasticNet(),param_grid=eNet_params).fit(X_lr_small_train,y_lr_small_train).best_estimator_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model score: 0.3019142149657966\n"
     ]
    }
   ],
   "source": [
    "bestModel=ElasticNet(l1_ratio=0.25,alpha=0.1)\n",
    "bestModel.fit(X_lr_small_train,y_lr_small_train)\n",
    "print('Best model score:',bestModel.score(X_lr_test,y_lr_test))\n",
    "#print('Best model coefficients:',bestModel.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.01892819, -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        , -0.02549994, -0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.01354898,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.02263579,  0.        , -0.01259263,  0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.        , -0.00761077,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.        ,  0.01479492, -0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.03286937,  0.        , -0.        ,\n",
       "        0.        ,  0.05342457,  0.        ,  0.01341571, -0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        , -0.        , -0.01913981, -0.        ,\n",
       "       -0.        ,  0.03673477, -0.        ,  0.        ,  0.        ,\n",
       "        0.00746342, -0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        ,  0.04629899,\n",
       "       -0.        , -0.        , -0.        , -0.        ,  0.043574  ,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.02815108,\n",
       "        0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.        , -0.08751661,\n",
       "       -0.        , -0.        , -0.        ,  0.        ,  0.01022868,\n",
       "       -0.        , -0.        ,  0.        , -0.        ,  0.01854745,\n",
       "       -0.11763832,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.04340677,  0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.0189911 ,  0.        ,  0.03960036,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        , -0.024536  ,\n",
       "        0.03714565,  0.0133049 , -0.        , -0.        , -0.        ,\n",
       "        0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        ,  0.        ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS_size_small= 100\n",
    "feature_lr_small=importantFeatures(bestModel,X_lr_small_train,y_lr_small_train,BS_size_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33., 34., 35., 36., 37., 38., 39.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostCommonFeatures(occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureIndex,occurrence=featureImportance(feature_lr_small)\n",
    "len(mostCommonFeatures(occurrence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15 important features\n",
      "[29. 30. 31. 32. 33. 34. 35. 36. 37. 39. 40. 41. 42. 43. 44.]\n",
      "There are 16 important features\n",
      "[37. 39. 40. 41. 42. 43. 44. 48. 60. 61. 62. 63. 64. 65. 66. 68.]\n",
      "There are 14 important features\n",
      "[33. 34. 35. 36. 37. 39. 40. 41. 42. 43. 44. 45. 60. 65.]\n",
      "There are 16 important features\n",
      "[30. 31. 32. 33. 34. 35. 36. 37. 39. 40. 41. 42. 43. 44. 60. 65.]\n",
      "There are 7 important features\n",
      "[37. 39. 40. 41. 42. 43. 47.]\n",
      "There are 6 important features\n",
      "[40. 41. 42. 43. 44. 65.]\n",
      "There are 9 important features\n",
      "[39. 40. 41. 42. 43. 44. 45. 47. 48.]\n",
      "There are 18 important features\n",
      "[33. 34. 35. 36. 37. 39. 40. 41. 42. 43. 44. 45. 47. 48. 60. 61. 62. 65.]\n",
      "There are 4 important features\n",
      "[43. 44. 45. 65.]\n",
      "There are 14 important features\n",
      "[29. 30. 31. 32. 33. 34. 35. 36. 37. 39. 40. 41. 42. 43.]\n",
      "[0.30830819 0.30395516 0.30417196 0.30256809 0.3075016  0.30291221\n",
      " 0.30661685 0.30036255 0.30442489 0.30593852]\n"
     ]
    }
   ],
   "source": [
    "#random missing and softImpute\n",
    "softImpute = SoftImpute()\n",
    "biscaler = BiScaler()\n",
    "softImputeScore=np.array([])\n",
    "scale=np.array([0.1,0.25,0.5,0.75])\n",
    "\n",
    "for i in range(10):\n",
    "    with HiddenPrints():\n",
    "        X_randomMissingSoft=compRandomFunc(X_lr_small_train,probability) #here is the line where you enable missing data\n",
    "        X_randomMissingSoft_normalized = biscaler.fit_transform(X_randomMissingSoft)\n",
    "        \n",
    "        lambda_0=np.nanmax(X_randomMissingSoft_normalized)\n",
    "        bestOfLambdaScore=np.array([])\n",
    "        for j in range(len(scale)):\n",
    "            softImpute=SoftImpute(shrinkage_value=lambda_0*scale[j])\n",
    "        \n",
    "            X_imputed_soft_normalized = softImpute.fit_transform(X_randomMissingSoft_normalized)\n",
    "            X_imputed_soft = biscaler.inverse_transform(X_imputed_soft_normalized)\n",
    "            \n",
    "            bestModel.fit(X_imputed_soft,y_lr_small_train)\n",
    "            bestOfLambdaScore=np.append(bestOfLambdaScore,bestModel.score(X_lr_test,y_lr_test))\n",
    "        \n",
    "        bestLambdaIndex=np.where(bestOfLambdaScore==np.max(bestOfLambdaScore))\n",
    "        softImpute=SoftImpute(shrinkage_value=lambda_0*scale[bestLambdaIndex])\n",
    "        X_imputed_soft_normalized = softImpute.fit_transform(X_randomMissingSoft_normalized)\n",
    "        X_imputed_soft = biscaler.inverse_transform(X_imputed_soft_normalized)\n",
    "        \n",
    "        softImputeScore=np.append(softImputeScore,np.max(bestOfLambdaScore))\n",
    "        \n",
    "   #do the feature selection thing\n",
    "    softImpute_feature=importantFeatures(bestModel,X_imputed_soft,y_lr_small_train,BS_size_small)\n",
    "    softImputed_featureIndex,softImputed_occurrence=featureImportance(softImpute_feature)\n",
    "    print('There are',len(mostCommonFeatures(softImputed_occurrence)),'important features',flush=True)\n",
    "    print(mostCommonFeatures(softImputed_occurrence),flush=True)\n",
    "print(softImputeScore,flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14 important features\n",
      "[32. 33. 34. 35. 36. 37. 39. 40. 41. 42. 43. 44. 60. 65.]\n",
      "There are 13 important features\n",
      "[31. 32. 33. 34. 35. 36. 37. 39. 40. 41. 42. 43. 44.]\n",
      "There are 12 important features\n",
      "[ 34.  35.  36.  37.  39.  40.  41.  42.  43.  44. 144.  60.]\n",
      "There are 12 important features\n",
      "[33. 34. 35. 36. 37. 39. 40. 41. 42. 43. 44. 70.]\n",
      "There are 20 important features\n",
      "[27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 39. 40. 41. 42. 43. 44. 45.\n",
      " 47. 70.]\n",
      "There are 17 important features\n",
      "[18. 19. 21. 22. 23. 24. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.]\n",
      "There are 14 important features\n",
      "[33. 34. 35. 36. 37. 39. 40. 41. 42. 43. 44. 45. 65. 84.]\n",
      "There are 13 important features\n",
      "[33. 34. 35. 36. 37. 39. 40. 41. 42. 43. 44. 45. 47.]\n",
      "There are 12 important features\n",
      "[33. 34. 35. 36. 37. 39. 40. 41. 42. 43. 44. 45.]\n",
      "There are 7 important features\n",
      "[40. 41. 42. 43. 44. 47. 65.]\n",
      "[0.30204085 0.30543181 0.30483037 0.30852851 0.3047836  0.30631347\n",
      " 0.30670643 0.3067765  0.30893385 0.3100666 ]\n"
     ]
    }
   ],
   "source": [
    "#softImpute with missing by feature\n",
    "softImputeMBFScore=np.array([])\n",
    "for i in range(10):\n",
    "    with HiddenPrints():\n",
    "        X_MBF_soft=missingByFeature(X_lr_small_train,p1,p2)\n",
    "        X_MBF_soft_normalized=biscaler.fit_transform(X_MBF_soft)\n",
    "        \n",
    "        lambda_0=np.nanmax(X_randomMissingSoft_normalized)\n",
    "        bestOfLambdaScore=np.array([])\n",
    "        for j in range(len(scale)):\n",
    "            softImpute=SoftImpute(shrinkage_value=lambda_0*scale[j])\n",
    "            \n",
    "            X_MBF_imputed_soft_normalized=softImpute.fit_transform(X_MBF_soft_normalized)\n",
    "            X_MBF_imputed_soft=biscaler.inverse_transform(X_MBF_imputed_soft_normalized)\n",
    "            \n",
    "            bestModel.fit(X_MBF_imputed_soft,y_lr_small_train)\n",
    "            bestOfLambdaScore=np.append(bestOfLambdaScore,bestModel.score(X_lr_test,y_lr_test))\n",
    "        \n",
    "        bestLambdaIndex=np.where(bestOfLambdaScore==np.max(bestOfLambdaScore))\n",
    "        softImpute=SoftImpute(shrinkage_value=lambda_0*scale[bestLambdaIndex])\n",
    "        \n",
    "        X_MBF_imputed_soft_normalized=softImpute.fit_transform(X_MBF_soft_normalized)\n",
    "        X_MBF_imputed_soft=biscaler.inverse_transform(X_MBF_imputed_soft_normalized)\n",
    "        \n",
    "        softImputeMBFScore=np.append(softImputeMBFScore,np.max(bestOfLambdaScore))\n",
    "    \n",
    "    #do the feature selection thing\n",
    "    softImpute_MBF_feature=importantFeatures(bestModel,X_MBF_imputed_soft,y_lr_small_train,BS_size_small)\n",
    "    softImputed_MBF_featureIndex,softImputed_MBF_occurrence=featureImportance(softImpute_MBF_feature)\n",
    "    \n",
    "    print('There are',len(mostCommonFeatures(softImputed_MBF_occurrence)),'important features')\n",
    "    print(mostCommonFeatures(softImputed_MBF_occurrence))\n",
    "print(softImputeMBFScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 important features\n",
      "[11. 12. 13. 15. 16. 17. 18. 19. 21. 22. 23.]\n",
      "There are 10 important features\n",
      "[ 9. 11. 12. 13. 15. 16. 17. 18. 19. 21.]\n",
      "There are 9 important features\n",
      "[12. 13. 15. 16. 17. 18. 19. 21. 22.]\n",
      "There are 11 important features\n",
      "[ 9. 11. 12. 13. 15. 16. 17. 18. 19. 21. 22.]\n",
      "There are 10 important features\n",
      "[ 9. 11. 12. 13. 15. 16. 17. 18. 19. 21.]\n",
      "There are 9 important features\n",
      "[12. 13. 15. 16. 17. 18. 19. 21. 23.]\n",
      "There are 11 important features\n",
      "[ 8.  9. 11. 12. 13. 15. 16. 17. 18. 19. 21.]\n",
      "There are 9 important features\n",
      "[11. 12. 13. 15. 16. 17. 18. 19. 21.]\n",
      "There are 12 important features\n",
      "[ 8.  9. 11. 12. 13. 15. 16. 17. 18. 19. 21. 22.]\n",
      "There are 10 important features\n",
      "[13. 15. 16. 17. 18. 19. 21. 22. 23. 27.]\n",
      "[-0.07266854 -0.0692111  -0.04265043 -0.0690027   0.03324835 -0.1615104\n",
      " -0.03423722 -0.19126057 -0.02648263 -0.00697168]\n"
     ]
    }
   ],
   "source": [
    "# random missing and mean impute\n",
    "imputeScore=np.array([])\n",
    "for i in range(10): \n",
    "    \n",
    "    missing_random=compRandomFunc(X_lr_small_train,probability)\n",
    "    imputed_random=imputetMissing(missing_random)\n",
    "    \n",
    "    #doing the predective performance thing\n",
    "    bestModel.fit(imputed_random,y_lr_small_train)\n",
    "    imputeScore=np.append(imputeScore,bestModel.score(X_lr_test,y_lr_test))\n",
    "    \n",
    "    #do the feature selection thing\n",
    "    imputed_feature=importantFeatures(bestModel,imputed_random,y_lr_small_train,BS_size_small)\n",
    "    imputed_featureIndex,imputed_occurrence=featureImportance(imputed_feature)\n",
    "    print('There are',len(mostCommonFeatures(imputed_occurrence)),'important features')\n",
    "    print(mostCommonFeatures(imputed_occurrence))\n",
    "    \n",
    "print(imputeScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14 important features\n",
      "[13. 15. 16. 17. 18. 19. 21. 22. 23. 27.]\n",
      "There are 13 important features\n",
      "[13. 15. 16. 17. 18. 19. 21. 22. 23. 27.]\n",
      "There are 15 important features\n",
      "[13. 15. 16. 17. 18. 19. 21. 22. 23. 27.]\n",
      "There are 12 important features\n",
      "[13. 15. 16. 17. 18. 19. 21. 22. 23. 27.]\n",
      "There are 14 important features\n",
      "[13. 15. 16. 17. 18. 19. 21. 22. 23. 27.]\n",
      "There are 16 important features\n",
      "[13. 15. 16. 17. 18. 19. 21. 22. 23. 27.]\n",
      "There are 12 important features\n",
      "[13. 15. 16. 17. 18. 19. 21. 22. 23. 27.]\n",
      "There are 11 important features\n",
      "[13. 15. 16. 17. 18. 19. 21. 22. 23. 27.]\n",
      "There are 15 important features\n",
      "[13. 15. 16. 17. 18. 19. 21. 22. 23. 27.]\n",
      "There are 15 important features\n",
      "[13. 15. 16. 17. 18. 19. 21. 22. 23. 27.]\n",
      "[0.30413935 0.28745913 0.2972928  0.291745   0.27229778 0.29709856\n",
      " 0.29682752 0.26692131 0.30170641 0.29271773]\n"
     ]
    }
   ],
   "source": [
    "MBF_imputeScore=np.array([])\n",
    "\n",
    "for i in range(10):\n",
    "    MBF_missing=missingByFeature(X_lr_small_train,p1,p2)\n",
    "    MBF_imputed=imputetMissing(MBF_missing)\n",
    "    \n",
    "    #doing the predective performace thing\n",
    "    bestModel.fit(MBF_imputed,y_lr_small_train)\n",
    "    MBF_imputeScore=np.append(MBF_imputeScore,bestModel.score(X_lr_test,y_lr_test))\n",
    "    \n",
    "    #do the feature selection thing\n",
    "    imputed_feature_MBF=importantFeatures(bestModel,MBF_imputed,y_lr_small_train,BS_size_small)\n",
    "    imputed_MBF_Index,imputed_MBF_occurrence=featureImportance(imputed_feature_MBF)\n",
    "    \n",
    "    print('There are',len(mostCommonFeatures(imputed_MBF_occurrence)),'important features')\n",
    "    print(mostCommonFeatures(imputed_occurrence))\n",
    "print(MBF_imputeScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
