{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "import random\n",
    "from sklearn.linear_model import Ridge\n",
    "from fancyimpute import SoftImpute, BiScaler\n",
    "from sklearn.linear_model import LinearRegression as reg\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "import itertools\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "import os, sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data=np.load('august2020-exercise1.npz')\n",
    "\n",
    "#print(data.files) # view the files in the npz\n",
    "\n",
    "#function to normalize the data\n",
    "def normalize(matrix):\n",
    "    normed=(matrix-matrix.mean(axis=0))/matrix.std(axis=0)\n",
    "    return normed\n",
    "\n",
    "######## train data ########\n",
    "X_lr_small_train=normalize(data['X_lr_small_train'])\n",
    "X_lr_big_train=normalize(data['X_lr_big_train'])\n",
    "X_hr_small_train=normalize(data['X_hr_small_train'])\n",
    "X_hr_big_train=normalize(data['X_hr_big_train'])\n",
    "\n",
    "y_lr_small_train=normalize(data['y_lr_small_train'])\n",
    "y_lr_big_train=normalize(data['y_lr_big_train'])\n",
    "y_hr_small_train=normalize(data['y_hr_small_train'])\n",
    "y_hr_big_train=normalize(data['y_hr_big_train'])\n",
    "######## test data#########\n",
    "X_lr_test=normalize(data['X_lr_test'])\n",
    "X_hr_test=normalize(data['X_hr_test'])\n",
    "\n",
    "y_lr_test=normalize(data['y_lr_test'])\n",
    "y_hr_test=normalize(data['y_hr_test'])\n",
    "\n",
    "def compRandomFunc(array,probability): # probability=0.7\n",
    "    n=array.shape[0]\n",
    "    p=array.shape[1]\n",
    "    array=array.astype(\"float\")\n",
    "    for i in range(n):    \n",
    "        for j in range(p):\n",
    "            r=random.uniform(0, 1)\n",
    "            if r < probability:\n",
    "                array[i,j]=np.NaN\n",
    "    return array\n",
    "\n",
    "def missingByFeature(array,p1,p2): #p1=0.875, p2=0.8\n",
    "    n=array.shape[0]\n",
    "    p=array.shape[1]\n",
    "    array=array.astype(\"float\")\n",
    "    featureList=np.array([])\n",
    "    for i in range(p):\n",
    "        r = random.uniform(0, 1)\n",
    "        if r<p1:\n",
    "            featureList=np.append(featureList,i)\n",
    "    for i in featureList:  # i is the counter for feature not sample size\n",
    "        i=int(i)\n",
    "        for j in range(n):\n",
    "            r2=random.uniform(0,1)\n",
    "            if r2<p2:\n",
    "                array[j,i]=np.NaN\n",
    "    return array\n",
    "\n",
    "def meanImputation(feature,data):\n",
    "    imputed=np.nanmean(data[:,feature])\n",
    "    return imputed\n",
    "\n",
    "# creating data with missing values\n",
    "import math\n",
    "probability=0.7\n",
    "p1=0.875\n",
    "p2=0.8\n",
    "# return the imputed matrix with mean imputation\n",
    "def imputetMissing(matrix):\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            if math.isnan(matrix[i,j]) is True:\n",
    "                matrix[i,j]=meanImputation(j,matrix)\n",
    "    return matrix\n",
    "\n",
    "def test(models, X_train,y_train,X_test,y_test, iterations = 100):\n",
    "    results = {}\n",
    "    for i in models:\n",
    "        r2_train = []\n",
    "        r2_test = []\n",
    "        for j in range(iterations):\n",
    "            r2_test.append(metrics.r2_score(y_test,\n",
    "                                            models[i].fit(X_train, \n",
    "                                                         y_train).predict(X_test)))\n",
    "\n",
    "            r2_train.append(metrics.r2_score(y_train, \n",
    "                                             models[i].fit(X_train, \n",
    "                                                          y_train).predict(X_train)))\n",
    "        results[i] = [np.mean(r2_train),np.mean(r2_test)]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def importantFeatures(best_model,X_train,y_train,BS_size):\n",
    "    importantFeatures=np.array([])\n",
    "    \n",
    "    for i in range(100):\n",
    "        #creatig bootstrap boots\n",
    "        x_bag=np.empty((0,X_train.shape[1]))\n",
    "        y_bag=np.array([])\n",
    "        for j in range(BS_size): \n",
    "            randIndex=np.random.randint(len(X_train), size=1)\n",
    "            x_sample=X_train[randIndex,:]\n",
    "            y_sample=y_train[randIndex]\n",
    "                \n",
    "            x_bag=np.concatenate((x_bag,x_sample))\n",
    "            y_bag=np.append(y_bag,y_sample)             \n",
    "        # test the estimator on bootstrap sample 100 times\n",
    "        #skippa grid search och kör dirr på bästa hyperparametrarna för att spara tid\n",
    "        bestEstimator=best_model.fit(x_bag,y_bag)\n",
    "        \n",
    "        rfeBest=RFE(bestEstimator)\n",
    "        X_rfe=rfeBest.fit_transform(x_bag,y_bag)\n",
    "        bestEstimator.fit(X_rfe,y_bag)\n",
    "        \n",
    "        importantFeatures=np.append(importantFeatures,np.where(rfeBest.ranking_==1))\n",
    "        \n",
    "    return importantFeatures\n",
    "\n",
    "def featureImportance(featureList):\n",
    "    unique_elements, counts_elements = np.unique(featureList, return_counts=True)\n",
    "    return unique_elements, counts_elements\n",
    "\n",
    "def mostCommonFeatures(occurrence):\n",
    "    mostCommonIndex=np.array([])\n",
    "    for i in range(len(occurrence)):\n",
    "        if occurrence[i]>=95:\n",
    "            mostCommonIndex=np.append(mostCommonIndex,[i])\n",
    "    mostCommonIndex=np.array(mostCommonIndex,dtype=np.int8)\n",
    "    mostCommonFeatures=featureIndex[mostCommonIndex]\n",
    "    return mostCommonFeatures\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high rate small data\n",
      "ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0.75,\n",
      "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "models = {'OLS': linear_model.LinearRegression(),\n",
    "         'Lasso': linear_model.Lasso(),\n",
    "         'Ridge': linear_model.Ridge(),}\n",
    "\n",
    "lasso_params = {'alpha':[0.02, 0.024, 0.025, 0.026, 0.03]}\n",
    "ridge_params = {'alpha':[200, 250, 300, 400, 500]}\n",
    "eNet_params={'alpha': [0, 0.5, 0.1, 0.01, 0.001],\n",
    "             'l1_ratio': [0, 0.25, 0.5, 0.75, 1]}\n",
    "\n",
    "models3 = {'OLS': linear_model.LinearRegression(),\n",
    "           'Lasso': GridSearchCV(linear_model.Lasso(), \n",
    "                               param_grid=lasso_params).fit(X_hr_small_train,y_hr_small_train).best_estimator_,\n",
    "           'Ridge': GridSearchCV(linear_model.Ridge(), \n",
    "                               param_grid=ridge_params).fit(X_hr_small_train,y_hr_small_train).best_estimator_,\n",
    "          'Elastic net':GridSearchCV(linear_model.ElasticNet(), \n",
    "                               param_grid=eNet_params).fit(X_hr_small_train,y_hr_small_train).best_estimator_}\n",
    "\n",
    "print('high rate small data')\n",
    "test(models3, X_hr_small_train,y_hr_small_train,X_hr_test,y_hr_test)\n",
    "print(GridSearchCV(linear_model.ElasticNet(),param_grid=eNet_params).fit(X_hr_small_train,y_hr_small_train).best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model score: 0.22250667272284674\n"
     ]
    }
   ],
   "source": [
    "bestModel=ElasticNet(l1_ratio=0.75,alpha=0.1)\n",
    "bestModel.fit(X_hr_small_train,y_hr_small_train)\n",
    "print('Best model score:',bestModel.score(X_hr_test,y_hr_test))\n",
    "#print('Best model coefficients:',bestModel.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "BS_size_small= 100\n",
    "feature_hr_small=importantFeatures(bestModel,X_hr_small_train,y_hr_small_train,BS_size_small)\n",
    "featureIndex,occurrence=featureImportance(feature_hr_small)\n",
    "print(len(mostCommonFeatures(occurrence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27 important features\n",
      "[22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39.\n",
      " 40. 76. 78. 79. 80. 81. 82. 83. 84.]\n",
      "There are 8 important features\n",
      "[33. 34. 35. 36. 37. 38. 39. 78.]\n",
      "There are 17 important features\n",
      "[24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40.]\n",
      "There are 6 important features\n",
      "[34. 35. 36. 37. 38. 39.]\n",
      "There are 19 important features\n",
      "[23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40.\n",
      " 80.]\n",
      "There are 16 important features\n",
      "[26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41.]\n",
      "There are 9 important features\n",
      "[33. 34. 35. 36. 37. 38. 39. 80. 81.]\n",
      "There are 20 important features\n",
      "[25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 81. 82. 83.\n",
      " 84. 85.]\n",
      "There are 21 important features\n",
      "[24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41.\n",
      " 42. 76. 79.]\n",
      "There are 20 important features\n",
      "[ 24.  25.  26.  27.  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.\n",
      "  38.  39. 127.  72.  73.  74.]\n",
      "[ 0.02270238  0.07761592  0.05759256  0.05817095  0.02684455  0.03558808\n",
      "  0.0753265  -0.00759354  0.10387669  0.06675995]\n"
     ]
    }
   ],
   "source": [
    "#random missing and softImpute\n",
    "softImpute = SoftImpute()\n",
    "biscaler = BiScaler()\n",
    "softImputeScore=np.array([])\n",
    "scale=np.array([0.1,0.25,0.5,0.75])\n",
    "\n",
    "for i in range(10):\n",
    "    with HiddenPrints():\n",
    "        X_randomMissingSoft=compRandomFunc(X_hr_small_train,probability) #here is the line where you enable missing data\n",
    "        X_randomMissingSoft_normalized = biscaler.fit_transform(X_randomMissingSoft)\n",
    "        \n",
    "        lambda_0=np.nanmax(X_randomMissingSoft_normalized)\n",
    "        bestOfLambdaScore=np.array([])\n",
    "        for j in range(len(scale)):\n",
    "            softImpute=SoftImpute(shrinkage_value=lambda_0*scale[j])\n",
    "        \n",
    "            X_imputed_soft_normalized = softImpute.fit_transform(X_randomMissingSoft_normalized)\n",
    "            X_imputed_soft = biscaler.inverse_transform(X_imputed_soft_normalized)\n",
    "            \n",
    "            bestModel.fit(X_imputed_soft,y_hr_small_train)\n",
    "            bestOfLambdaScore=np.append(bestOfLambdaScore,bestModel.score(X_hr_test,y_hr_test))\n",
    "        \n",
    "        bestLambdaIndex=np.where(bestOfLambdaScore==np.max(bestOfLambdaScore))\n",
    "        softImpute=SoftImpute(shrinkage_value=lambda_0*scale[bestLambdaIndex])\n",
    "        X_imputed_soft_normalized = softImpute.fit_transform(X_randomMissingSoft_normalized)\n",
    "        X_imputed_soft = biscaler.inverse_transform(X_imputed_soft_normalized)\n",
    "        \n",
    "        softImputeScore=np.append(softImputeScore,np.max(bestOfLambdaScore))\n",
    "        \n",
    "   #do the feature selection thing\n",
    "    softImpute_feature=importantFeatures(bestModel,X_imputed_soft,y_hr_small_train,BS_size_small)\n",
    "    softImputed_featureIndex,softImputed_occurrence=featureImportance(softImpute_feature)\n",
    "    print('There are',len(mostCommonFeatures(softImputed_occurrence)),'important features',flush=True)\n",
    "    print(mostCommonFeatures(softImputed_occurrence),flush=True)\n",
    "print(softImputeScore,flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25 important features\n",
      "[21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38.\n",
      " 39. 40. 41. 42. 43. 81. 82.]\n",
      "There are 18 important features\n",
      "[27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 75. 76. 77. 78. 79.]\n",
      "There are 18 important features\n",
      "[34. 35. 36. 37. 38. 39. 40. 74. 76. 77. 78. 79. 80. 81. 82. 83. 84. 86.]\n",
      "There are 21 important features\n",
      "[28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 84. 87. 88. 89.\n",
      " 91. 92. 93.]\n",
      "There are 20 important features\n",
      "[22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39.\n",
      " 80. 81.]\n",
      "There are 28 important features\n",
      "[15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32.\n",
      " 33. 34. 35. 36. 37. 38. 39. 40. 81. 85.]\n",
      "There are 21 important features\n",
      "[24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41.\n",
      " 42. 43. 92.]\n",
      "There are 21 important features\n",
      "[29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 43. 73. 74. 75. 76. 77. 78.\n",
      " 79. 81. 82.]\n",
      "There are 21 important features\n",
      "[30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 72. 73. 74. 75. 76. 77.\n",
      " 78. 79. 84.]\n",
      "There are 24 important features\n",
      "[23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40.\n",
      " 42. 43. 76. 80. 81. 83.]\n",
      "[ 0.01882649  0.07014477  0.0923292   0.10634195  0.05639917  0.04372489\n",
      "  0.14383206  0.01499552  0.03633681 -0.00366374]\n"
     ]
    }
   ],
   "source": [
    "#softImpute with missing by feature\n",
    "softImputeMBFScore=np.array([])\n",
    "for i in range(10):\n",
    "    with HiddenPrints():\n",
    "        X_MBF_soft=missingByFeature(X_hr_small_train,p1,p2)\n",
    "        X_MBF_soft_normalized=biscaler.fit_transform(X_MBF_soft)\n",
    "        \n",
    "        lambda_0=np.nanmax(X_randomMissingSoft_normalized)\n",
    "        bestOfLambdaScore=np.array([])\n",
    "        for j in range(len(scale)):\n",
    "            softImpute=SoftImpute(shrinkage_value=lambda_0*scale[j])\n",
    "            \n",
    "            X_MBF_imputed_soft_normalized=softImpute.fit_transform(X_MBF_soft_normalized)\n",
    "            X_MBF_imputed_soft=biscaler.inverse_transform(X_MBF_imputed_soft_normalized)\n",
    "            \n",
    "            bestModel.fit(X_MBF_imputed_soft,y_hr_small_train)\n",
    "            bestOfLambdaScore=np.append(bestOfLambdaScore,bestModel.score(X_hr_test,y_hr_test))\n",
    "        \n",
    "        bestLambdaIndex=np.where(bestOfLambdaScore==np.max(bestOfLambdaScore))\n",
    "        softImpute=SoftImpute(shrinkage_value=lambda_0*scale[bestLambdaIndex])\n",
    "        \n",
    "        X_MBF_imputed_soft_normalized=softImpute.fit_transform(X_MBF_soft_normalized)\n",
    "        X_MBF_imputed_soft=biscaler.inverse_transform(X_MBF_imputed_soft_normalized)\n",
    "        \n",
    "        softImputeMBFScore=np.append(softImputeMBFScore,np.max(bestOfLambdaScore))\n",
    "    \n",
    "    #do the feature selection thing\n",
    "    softImpute_MBF_feature=importantFeatures(bestModel,X_MBF_imputed_soft,y_hr_small_train,BS_size_small)\n",
    "    softImputed_MBF_featureIndex,softImputed_MBF_occurrence=featureImportance(softImpute_MBF_feature)\n",
    "    \n",
    "    print('There are',len(mostCommonFeatures(softImputed_MBF_occurrence)),'important features')\n",
    "    print(mostCommonFeatures(softImputed_MBF_occurrence))\n",
    "print(softImputeMBFScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestLambdaIndex[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16 important features\n",
      "[25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 79.]\n",
      "There are 20 important features\n",
      "[22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39.\n",
      " 40. 80.]\n",
      "There are 8 important features\n",
      "[34. 35. 36. 37. 38. 39. 40. 81.]\n",
      "There are 16 important features\n",
      "[23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38.]\n",
      "There are 19 important features\n",
      "[23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40.\n",
      " 41.]\n",
      "There are 18 important features\n",
      "[23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40.]\n",
      "There are 19 important features\n",
      "[22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39.\n",
      " 40.]\n",
      "There are 9 important features\n",
      "[32. 33. 34. 35. 36. 37. 38. 39. 81.]\n",
      "There are 22 important features\n",
      "[27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 75. 79. 80.\n",
      " 81. 82. 83. 84.]\n",
      "There are 21 important features\n",
      "[32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 74. 75. 76. 77. 78. 79. 80. 81.\n",
      " 82. 83. 84.]\n",
      "[0.04545966 0.02966838 0.01276285 0.01337962 0.02167668 0.00907319\n",
      " 0.02824029 0.03344825 0.04892604 0.0502073 ]\n"
     ]
    }
   ],
   "source": [
    "# random missing and mean impute\n",
    "imputeScore=np.array([])\n",
    "for i in range(10): \n",
    "    \n",
    "    missing_random=compRandomFunc(X_hr_small_train,probability)\n",
    "    imputed_random=imputetMissing(missing_random)\n",
    "    \n",
    "    #doing the predective performance thing\n",
    "    bestModel.fit(imputed_random,y_hr_small_train)\n",
    "    imputeScore=np.append(imputeScore,bestModel.score(X_hr_test,y_hr_test))\n",
    "    \n",
    "    #do the feature selection thing\n",
    "    imputed_feature=importantFeatures(bestModel,imputed_random,y_hr_small_train,BS_size_small)\n",
    "    imputed_featureIndex,imputed_occurrence=featureImportance(imputed_feature)\n",
    "    print('There are',len(mostCommonFeatures(imputed_occurrence)),'important features')\n",
    "    print(mostCommonFeatures(imputed_occurrence))\n",
    "    \n",
    "print(imputeScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26 important features\n",
      "[32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 74. 75. 76. 77. 78. 79. 80. 81.\n",
      " 82. 83. 84.]\n",
      "There are 4 important features\n",
      "[32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 74. 75. 76. 77. 78. 79. 80. 81.\n",
      " 82. 83. 84.]\n",
      "There are 24 important features\n",
      "[32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 74. 75. 76. 77. 78. 79. 80. 81.\n",
      " 82. 83. 84.]\n",
      "There are 2 important features\n",
      "[32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 74. 75. 76. 77. 78. 79. 80. 81.\n",
      " 82. 83. 84.]\n",
      "There are 30 important features\n",
      "[32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 74. 75. 76. 77. 78. 79. 80. 81.\n",
      " 82. 83. 84.]\n",
      "There are 16 important features\n",
      "[32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 74. 75. 76. 77. 78. 79. 80. 81.\n",
      " 82. 83. 84.]\n",
      "There are 33 important features\n",
      "[32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 74. 75. 76. 77. 78. 79. 80. 81.\n",
      " 82. 83. 84.]\n",
      "There are 23 important features\n",
      "[32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 74. 75. 76. 77. 78. 79. 80. 81.\n",
      " 82. 83. 84.]\n",
      "There are 16 important features\n",
      "[32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 74. 75. 76. 77. 78. 79. 80. 81.\n",
      " 82. 83. 84.]\n",
      "There are 15 important features\n",
      "[32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 74. 75. 76. 77. 78. 79. 80. 81.\n",
      " 82. 83. 84.]\n",
      "[0.0045342  0.06855639 0.0003127  0.04901418 0.05181028 0.00069362\n",
      " 0.02232381 0.09973168 0.04678719 0.09465881]\n"
     ]
    }
   ],
   "source": [
    "MBF_imputeScore=np.array([])\n",
    "\n",
    "for i in range(10):\n",
    "    MBF_missing=missingByFeature(X_hr_small_train,p1,p2)\n",
    "    MBF_imputed=imputetMissing(MBF_missing)\n",
    "    \n",
    "    #doing the predective performace thing\n",
    "    bestModel.fit(MBF_imputed,y_hr_small_train)\n",
    "    MBF_imputeScore=np.append(MBF_imputeScore,bestModel.score(X_hr_test,y_hr_test))\n",
    "    \n",
    "    #do the feature selection thing\n",
    "    imputed_feature_MBF=importantFeatures(bestModel,MBF_imputed,y_hr_small_train,BS_size_small)\n",
    "    imputed_MBF_Index,imputed_MBF_occurrence=featureImportance(imputed_feature_MBF)\n",
    "    \n",
    "    print('There are',len(mostCommonFeatures(imputed_MBF_occurrence)),'important features')\n",
    "    print(mostCommonFeatures(imputed_occurrence))\n",
    "print(MBF_imputeScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
