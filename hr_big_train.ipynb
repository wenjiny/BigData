{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "import random\n",
    "from sklearn.linear_model import Ridge\n",
    "from fancyimpute import SoftImpute, BiScaler\n",
    "from sklearn.linear_model import LinearRegression as reg\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "import itertools\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "import os, sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data=np.load('august2020-exercise1.npz')\n",
    "\n",
    "#print(data.files) # view the files in the npz\n",
    "\n",
    "#function to normalize the data\n",
    "def normalize(matrix):\n",
    "    normed=(matrix-matrix.mean(axis=0))/matrix.std(axis=0)\n",
    "    return normed\n",
    "\n",
    "######## train data ########\n",
    "X_lr_small_train=normalize(data['X_lr_small_train'])\n",
    "X_lr_big_train=normalize(data['X_lr_big_train'])\n",
    "X_hr_small_train=normalize(data['X_hr_small_train'])\n",
    "X_hr_big_train=normalize(data['X_hr_big_train'])\n",
    "\n",
    "y_lr_small_train=normalize(data['y_lr_small_train'])\n",
    "y_lr_big_train=normalize(data['y_lr_big_train'])\n",
    "y_hr_small_train=normalize(data['y_hr_small_train'])\n",
    "y_hr_big_train=normalize(data['y_hr_big_train'])\n",
    "######## test data#########\n",
    "X_lr_test=normalize(data['X_lr_test'])\n",
    "X_hr_test=normalize(data['X_hr_test'])\n",
    "\n",
    "y_lr_test=normalize(data['y_lr_test'])\n",
    "y_hr_test=normalize(data['y_hr_test'])\n",
    "\n",
    "def compRandomFunc(array,probability): # probability=0.7\n",
    "    n=array.shape[0]\n",
    "    p=array.shape[1]\n",
    "    array=array.astype(\"float\")\n",
    "    for i in range(n):    \n",
    "        for j in range(p):\n",
    "            r=random.uniform(0, 1)\n",
    "            if r < probability:\n",
    "                array[i,j]=np.NaN\n",
    "    return array\n",
    "\n",
    "def missingByFeature(array,p1,p2): #p1=0.875, p2=0.8\n",
    "    n=array.shape[0]\n",
    "    p=array.shape[1]\n",
    "    array=array.astype(\"float\")\n",
    "    featureList=np.array([])\n",
    "    for i in range(p):\n",
    "        r = random.uniform(0, 1)\n",
    "        if r<p1:\n",
    "            featureList=np.append(featureList,i)\n",
    "    for i in featureList:  # i is the counter for feature not sample size\n",
    "        i=int(i)\n",
    "        for j in range(n):\n",
    "            r2=random.uniform(0,1)\n",
    "            if r2<p2:\n",
    "                array[j,i]=np.NaN\n",
    "    return array\n",
    "\n",
    "def meanImputation(feature,data):\n",
    "    imputed=np.nanmean(data[:,feature])\n",
    "    return imputed\n",
    "\n",
    "# creating data with missing values\n",
    "import math\n",
    "probability=0.7\n",
    "p1=0.875\n",
    "p2=0.8\n",
    "# return the imputed matrix with mean imputation\n",
    "def imputetMissing(matrix):\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            if math.isnan(matrix[i,j]) is True:\n",
    "                matrix[i,j]=meanImputation(j,matrix)\n",
    "    return matrix\n",
    "\n",
    "def test(models, X_train,y_train,X_test,y_test, iterations = 100):\n",
    "    results = {}\n",
    "    for i in models:\n",
    "        r2_train = []\n",
    "        r2_test = []\n",
    "        for j in range(iterations):\n",
    "            r2_test.append(metrics.r2_score(y_test,\n",
    "                                            models[i].fit(X_train, \n",
    "                                                         y_train).predict(X_test)))\n",
    "\n",
    "            r2_train.append(metrics.r2_score(y_train, \n",
    "                                             models[i].fit(X_train, \n",
    "                                                          y_train).predict(X_train)))\n",
    "        results[i] = [np.mean(r2_train),np.mean(r2_test)]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def importantFeatures(best_model,X_train,y_train,BS_size):\n",
    "    importantFeatures=np.array([])\n",
    "    \n",
    "    for i in range(100):\n",
    "        #creatig bootstrap boots\n",
    "        x_bag=np.empty((0,X_train.shape[1]))\n",
    "        y_bag=np.array([])\n",
    "        for j in range(BS_size): \n",
    "            randIndex=np.random.randint(len(X_train), size=1)\n",
    "            x_sample=X_train[randIndex,:]\n",
    "            y_sample=y_train[randIndex]\n",
    "                \n",
    "            x_bag=np.concatenate((x_bag,x_sample))\n",
    "            y_bag=np.append(y_bag,y_sample)             \n",
    "        # test the estimator on bootstrap sample 100 times\n",
    "        #skippa grid search och kör dirr på bästa hyperparametrarna för att spara tid\n",
    "        bestEstimator=best_model.fit(x_bag,y_bag)\n",
    "        \n",
    "        rfeBest=RFE(bestEstimator)\n",
    "        X_rfe=rfeBest.fit_transform(x_bag,y_bag)\n",
    "        bestEstimator.fit(X_rfe,y_bag)\n",
    "        \n",
    "        importantFeatures=np.append(importantFeatures,np.where(rfeBest.ranking_==1))\n",
    "        \n",
    "    return importantFeatures\n",
    "\n",
    "def featureImportance(featureList):\n",
    "    unique_elements, counts_elements = np.unique(featureList, return_counts=True)\n",
    "    return unique_elements, counts_elements\n",
    "\n",
    "def mostCommonFeatures(occurrence):\n",
    "    mostCommonIndex=np.array([])\n",
    "    for i in range(len(occurrence)):\n",
    "        if occurrence[i]>=80:\n",
    "            mostCommonIndex=np.append(mostCommonIndex,[i])\n",
    "    mostCommonIndex=np.array(mostCommonIndex,dtype=np.int8)\n",
    "    mostCommonFeatures=featureIndex[mostCommonIndex]\n",
    "    return mostCommonFeatures\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high rate big data\n",
      "ElasticNet(alpha=0.01, copy_X=True, fit_intercept=True, l1_ratio=1,\n",
      "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "models = {'OLS': linear_model.LinearRegression(),\n",
    "         'Lasso': linear_model.Lasso(),\n",
    "         'Ridge': linear_model.Ridge(),}\n",
    "\n",
    "lasso_params = {'alpha':[0.02, 0.024, 0.025, 0.026, 0.03]}\n",
    "ridge_params = {'alpha':[200, 250, 300, 400, 500]}\n",
    "eNet_params={'alpha': [0, 0.5, 0.1, 0.01, 0.001],\n",
    "             'l1_ratio': [0, 0.25, 0.5, 0.75, 1]}\n",
    "\n",
    "#grid search for high rate big data\n",
    "models4 = {'OLS': linear_model.LinearRegression(),\n",
    "           'Lasso': GridSearchCV(linear_model.Lasso(), \n",
    "                               param_grid=lasso_params).fit(X_hr_big_train,y_hr_big_train).best_estimator_,\n",
    "           'Ridge': GridSearchCV(linear_model.Ridge(), \n",
    "                               param_grid=ridge_params).fit(X_hr_big_train,y_hr_big_train).best_estimator_,\n",
    "          'Elastic net':GridSearchCV(linear_model.ElasticNet(), \n",
    "                               param_grid=eNet_params).fit(X_hr_big_train,y_hr_big_train).best_estimator_}\n",
    "\n",
    "print('high rate big data')\n",
    "test(models4, X_hr_big_train,y_hr_big_train,X_hr_test,y_hr_test)\n",
    "print(GridSearchCV(linear_model.ElasticNet(),param_grid=eNet_params).fit(X_hr_big_train,y_hr_big_train).best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model score: 0.3277928784922103\n"
     ]
    }
   ],
   "source": [
    "bestModel=ElasticNet(l1_ratio=1,alpha=0.01)\n",
    "bestModel.fit(X_hr_big_train,y_hr_big_train)\n",
    "print('Best model score:',bestModel.score(X_hr_test,y_hr_test))\n",
    "#print('Best model coefficients:',bestModel.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.98059472e-03,  2.97842846e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  1.11043853e-03,  0.00000000e+00,\n",
       "        0.00000000e+00,  7.15773296e-03,  9.28168764e-03, -0.00000000e+00,\n",
       "        0.00000000e+00, -6.22823546e-03, -8.60493095e-03, -0.00000000e+00,\n",
       "        0.00000000e+00,  2.76826628e-02, -0.00000000e+00,  2.73932387e-02,\n",
       "       -7.18300214e-03, -4.54331138e-03,  6.71100963e-02,  0.00000000e+00,\n",
       "       -2.22491822e-03, -8.01775808e-03,  0.00000000e+00,  1.05994721e-02,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        6.05679793e-02,  0.00000000e+00, -2.13372586e-02, -2.66520158e-04,\n",
       "       -8.92456517e-03,  2.47391817e-04,  0.00000000e+00,  1.11555115e-01,\n",
       "        5.52255814e-03, -0.00000000e+00,  0.00000000e+00,  7.25237241e-02,\n",
       "        1.00403524e-02, -2.24215480e-03,  0.00000000e+00, -2.96248255e-03,\n",
       "       -3.79479840e-03,  7.99407915e-02,  0.00000000e+00, -5.09904945e-03,\n",
       "        9.75704921e-04, -4.94701010e-03, -6.85028047e-03,  5.22019798e-03,\n",
       "       -0.00000000e+00,  1.03245041e-03, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -3.37093730e-03, -7.92166670e-03,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -9.68237551e-03,  3.15149124e-03,  0.00000000e+00,\n",
       "       -2.40038968e-03,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  4.47313984e-03,\n",
       "        0.00000000e+00, -9.45368446e-04, -0.00000000e+00,  3.06708813e-02,\n",
       "        4.84042722e-03, -3.94075501e-03, -1.40391133e-02, -5.42421260e-03,\n",
       "        0.00000000e+00,  1.03003431e-02,  7.11842427e-03, -0.00000000e+00,\n",
       "       -4.24994914e-03,  1.43422472e-01,  0.00000000e+00,  7.30202927e-03,\n",
       "        0.00000000e+00,  2.25308744e-03, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  2.16255092e-01,  0.00000000e+00,\n",
       "       -0.00000000e+00,  5.31817471e-03, -8.44112458e-03,  1.11260205e-03,\n",
       "        1.04669018e-04, -0.00000000e+00,  7.13295289e-03, -8.83132635e-04,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -3.01565406e-03,\n",
       "        7.11949128e-03, -1.12882029e-03, -2.26082829e-03, -0.00000000e+00,\n",
       "        0.00000000e+00, -1.01122880e-03,  6.57278827e-03,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  1.07561963e-02,  8.21774044e-03,\n",
       "       -0.00000000e+00,  1.67141558e-01, -0.00000000e+00,  4.64861814e-02,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -3.91103857e-03,\n",
       "       -1.24991971e-04, -0.00000000e+00, -0.00000000e+00,  4.10594376e-03,\n",
       "        2.60923763e-01, -0.00000000e+00, -0.00000000e+00, -1.63028783e-03,\n",
       "       -0.00000000e+00,  0.00000000e+00,  2.69270882e-02,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -3.01569715e-03, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -1.02787447e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  1.43210321e-03,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  1.42125069e-02,\n",
       "        1.56773877e-01, -4.43449057e-03,  4.13461798e-02, -0.00000000e+00,\n",
       "        2.49604597e-01, -3.16506183e-03, -1.19597490e-02,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -1.75426302e-02,  1.13465246e-02, -0.00000000e+00,\n",
       "        4.86560949e-03,  0.00000000e+00, -0.00000000e+00,  8.32830445e-03,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  1.59264833e-05, -9.69004972e-04,  0.00000000e+00])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 22.  32.  39.  43.  49.  97. 110.  81.  83.  92. 116. 118. 120.]\n"
     ]
    }
   ],
   "source": [
    "BS_size_big=1000\n",
    "feature_hr_big=importantFeatures(bestModel,X_hr_big_train,y_hr_big_train,BS_size_big)\n",
    "featureIndex,occurrence=featureImportance(feature_hr_big)\n",
    "print(mostCommonFeatures(occurrence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mostCommonFeatures(occurrence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 important features\n",
      "[ 22.  39.  43.  97. 110.  81.  92. 116. 120.]\n",
      "There are 8 important features\n",
      "[ 39.  97. 110.  81.  92. 112. 116. 120.]\n",
      "There are 9 important features\n",
      "[ 39.  49.  97. 110.  81.  83.  92. 116. 120.]\n",
      "There are 9 important features\n",
      "[ 19.  39.  97. 110.  81.  92.  93. 116. 120.]\n",
      "There are 8 important features\n",
      "[ 39.  49.  97. 110.  81.  92. 116. 120.]\n",
      "There are 9 important features\n",
      "[ 22.  39.  49.  97. 110.  81.  92. 116. 120.]\n",
      "There are 7 important features\n",
      "[ 39.  97. 110.  81.  92. 116. 120.]\n",
      "There are 7 important features\n",
      "[ 39.  97. 110.  81.  92. 116. 120.]\n",
      "There are 8 important features\n",
      "[  0.  39.  97. 110.  81.  92. 116. 120.]\n",
      "There are 8 important features\n",
      "[ 39.  43.  97. 110.  81.  92. 116. 120.]\n",
      "[0.28863611 0.27516068 0.26625792 0.28166749 0.28822205 0.29232699\n",
      " 0.28869516 0.28115911 0.28970304 0.2863328 ]\n"
     ]
    }
   ],
   "source": [
    "#random missing and softImpute\n",
    "softImpute = SoftImpute()\n",
    "biscaler = BiScaler()\n",
    "softImputeScore=np.array([])\n",
    "scale=np.array([0.1,0.25,0.5,0.75])\n",
    "\n",
    "for i in range(10):\n",
    "    with HiddenPrints():\n",
    "        X_randomMissingSoft=compRandomFunc(X_hr_big_train,probability) #here is the line where you enable missing data\n",
    "        X_randomMissingSoft_normalized = biscaler.fit_transform(X_randomMissingSoft)\n",
    "        \n",
    "        lambda_0=np.nanmax(X_randomMissingSoft_normalized)\n",
    "        bestOfLambdaScore=np.array([])\n",
    "        for j in range(len(scale)):\n",
    "            softImpute=SoftImpute(shrinkage_value=lambda_0*scale[j])\n",
    "            \n",
    "            X_imputed_soft_normalized = softImpute.fit_transform(X_randomMissingSoft_normalized)\n",
    "            X_imputed_soft = biscaler.inverse_transform(X_imputed_soft_normalized)\n",
    "            \n",
    "            bestModel.fit(X_imputed_soft,y_hr_big_train)\n",
    "            bestOfLambdaScore=np.append(bestOfLambdaScore,bestModel.score(X_hr_test,y_hr_test))\n",
    "            \n",
    "        bestLambdaIndex=np.where(bestOfLambdaScore==np.max(bestOfLambdaScore))\n",
    "        softImpute=SoftImpute(shrinkage_value=lambda_0*scale[bestLambdaIndex])\n",
    "        X_imputed_soft_normalized = softImpute.fit_transform(X_randomMissingSoft_normalized)\n",
    "        X_imputed_soft = biscaler.inverse_transform(X_imputed_soft_normalized)\n",
    "        \n",
    "        softImputeScore=np.append(softImputeScore,np.max(bestOfLambdaScore))\n",
    "        \n",
    "    #do the feature selection thing\n",
    "    softImpute_feature=importantFeatures(bestModel,X_imputed_soft,y_hr_big_train,BS_size_big)\n",
    "    softImputed_featureIndex,softImputed_occurrence=featureImportance(softImpute_feature)\n",
    "    print('There are',len(mostCommonFeatures(softImputed_occurrence)),'important features',flush=True)\n",
    "    print(mostCommonFeatures(softImputed_occurrence),flush=True)\n",
    "print(softImputeScore,flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 important features\n",
      "[ 14.  31.  33.  49.  68.  73.  97. 110.  81.  92. 116. 120. 134.]\n",
      "There are 16 important features\n",
      "[  4.   6.   7.  11.  19.  39.  52.  67.  70. 110.  81.  92. 120. 134.\n",
      " 135. 140.]\n",
      "There are 12 important features\n",
      "[ 21.  22.  33.  39.  49.  67. 102. 110.  81.  92. 116. 120.]\n",
      "There are 16 important features\n",
      "[ 22.  25.  43.  49.  61.  70.  89.  96.  97. 101. 110.  81.  92. 116.\n",
      " 120. 142.]\n",
      "There are 12 important features\n",
      "[  1.   5.   9.  22. 110.  79.  81.  92. 105. 116. 120. 130.]\n",
      "There are 18 important features\n",
      "[  3.   5.   6.   7.   8.  19.  39.  47.  49.  89.  97. 110.  81.  92.\n",
      " 113. 116. 120. 134.]\n",
      "There are 16 important features\n",
      "[  2.   4.   5.   6.   7.   9.  39.  43.  53.  97. 110.  81.  92. 116.\n",
      " 120. 130.]\n",
      "There are 12 important features\n",
      "[  7.   8.  22.  49.  97. 110.  81.  92. 116. 118. 120. 134.]\n",
      "There are 14 important features\n",
      "[ 21.  31.  39.  49.  70.  82.  90.  97. 110. 127.  81.  83.  92. 120.]\n",
      "There are 11 important features\n",
      "[  7.  33.  97. 110.  72.  81.  92. 106. 107. 116. 120.]\n",
      "[0.26246809 0.26508272 0.28350317 0.280454   0.27696664 0.27169784\n",
      " 0.2715403  0.279255   0.25618075 0.25394389]\n"
     ]
    }
   ],
   "source": [
    "#softImpute with missing by feature\n",
    "softImputeMBFScore=np.array([])\n",
    "for i in range(10):\n",
    "    with HiddenPrints():\n",
    "        X_MBF_soft=missingByFeature(X_hr_big_train,p1,p2)\n",
    "        X_MBF_soft_normalized=biscaler.fit_transform(X_MBF_soft)\n",
    "        \n",
    "        lambda_0=np.nanmax(X_randomMissingSoft_normalized)\n",
    "        bestOfLambdaScore=np.array([])\n",
    "        for j in range(len(scale)):\n",
    "            softImpute=SoftImpute(shrinkage_value=lambda_0*scale[j])\n",
    "            \n",
    "            X_MBF_imputed_soft_normalized=softImpute.fit_transform(X_MBF_soft_normalized)\n",
    "            X_MBF_imputed_soft=biscaler.inverse_transform(X_MBF_imputed_soft_normalized)\n",
    "            \n",
    "            bestModel.fit(X_MBF_imputed_soft,y_hr_big_train)\n",
    "            bestOfLambdaScore=np.append(bestOfLambdaScore,bestModel.score(X_hr_test,y_hr_test))\n",
    "        \n",
    "        bestLambdaIndex=np.where(bestOfLambdaScore==np.max(bestOfLambdaScore))\n",
    "        softImpute=SoftImpute(shrinkage_value=lambda_0*scale[bestLambdaIndex])\n",
    "        \n",
    "        X_MBF_imputed_soft_normalized=softImpute.fit_transform(X_MBF_soft_normalized)\n",
    "        X_MBF_imputed_soft=biscaler.inverse_transform(X_MBF_imputed_soft_normalized)\n",
    "        \n",
    "        softImputeMBFScore=np.append(softImputeMBFScore,np.max(bestOfLambdaScore))\n",
    "    \n",
    "    #do the feature selection thing\n",
    "    softImpute_MBF_feature=importantFeatures(bestModel,X_MBF_imputed_soft,y_hr_big_train,BS_size_big)\n",
    "    softImputed_MBF_featureIndex,softImputed_MBF_occurrence=featureImportance(softImpute_MBF_feature)\n",
    "    \n",
    "    print('There are',len(mostCommonFeatures(softImputed_MBF_occurrence)),'important features')\n",
    "    print(mostCommonFeatures(softImputed_MBF_occurrence))\n",
    "print(softImputeMBFScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 important features\n",
      "[ 39.  43.  49.  70.  97. 110.  81.  92. 116. 120. 130.]\n",
      "There are 10 important features\n",
      "[ 19.  39.  43.  49.  97. 110.  81.  92. 116. 120.]\n",
      "There are 9 important features\n",
      "[ 19.  39.  49.  97. 110.  81.  92. 116. 120.]\n",
      "There are 11 important features\n",
      "[ 19.  39.  49.  97. 110.  81.  92. 106. 116. 120. 134.]\n",
      "There are 9 important features\n",
      "[ 39.  49.  97. 110.  81.  84.  92. 116. 120.]\n",
      "There are 9 important features\n",
      "[ 39.  49.  97. 110.  81.  92. 116. 120. 131.]\n",
      "There are 9 important features\n",
      "[ 39.  43.  49.  97. 110.  81.  92. 116. 120.]\n",
      "There are 9 important features\n",
      "[ 39.  49.  97. 110.  81.  92. 116. 120. 130.]\n",
      "There are 9 important features\n",
      "[ 39.  43.  49.  97. 110.  81.  92. 116. 120.]\n",
      "There are 7 important features\n",
      "[ 39.  97. 110.  81.  92. 116. 120.]\n",
      "[0.30221332 0.29390071 0.28487832 0.28605767 0.29204145 0.29054609\n",
      " 0.28945875 0.30015482 0.29657769 0.29846784]\n"
     ]
    }
   ],
   "source": [
    "# random missing and mean impute\n",
    "imputeScore=np.array([])\n",
    "for i in range(10): \n",
    "    \n",
    "    missing_random=compRandomFunc(X_hr_big_train,probability)\n",
    "    imputed_random=imputetMissing(missing_random)\n",
    "    \n",
    "    #doing the predective performance thing\n",
    "    bestModel.fit(imputed_random,y_hr_big_train)\n",
    "    imputeScore=np.append(imputeScore,bestModel.score(X_hr_test,y_hr_test))\n",
    "    \n",
    "    #do the feature selection thing\n",
    "    imputed_feature=importantFeatures(bestModel,imputed_random,y_hr_big_train,BS_size_big)\n",
    "    imputed_featureIndex,imputed_occurrence=featureImportance(imputed_feature)\n",
    "    print('There are',len(mostCommonFeatures(imputed_occurrence)),'important features')\n",
    "    print(mostCommonFeatures(imputed_occurrence))\n",
    "    \n",
    "print(imputeScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 important features\n",
      "[ 39.  97. 110.  81.  92. 116. 120.]\n",
      "There are 12 important features\n",
      "[ 39.  97. 110.  81.  92. 116. 120.]\n",
      "There are 13 important features\n",
      "[ 39.  97. 110.  81.  92. 116. 120.]\n",
      "There are 12 important features\n",
      "[ 39.  97. 110.  81.  92. 116. 120.]\n",
      "There are 11 important features\n",
      "[ 39.  97. 110.  81.  92. 116. 120.]\n",
      "There are 11 important features\n",
      "[ 39.  97. 110.  81.  92. 116. 120.]\n",
      "There are 11 important features\n",
      "[ 39.  97. 110.  81.  92. 116. 120.]\n",
      "There are 13 important features\n",
      "[ 39.  97. 110.  81.  92. 116. 120.]\n",
      "There are 8 important features\n",
      "[ 39.  97. 110.  81.  92. 116. 120.]\n",
      "There are 12 important features\n",
      "[ 39.  97. 110.  81.  92. 116. 120.]\n",
      "[0.27584298 0.28398105 0.28944321 0.28404941 0.27537315 0.2787337\n",
      " 0.28339885 0.28706989 0.28912546 0.2873552 ]\n"
     ]
    }
   ],
   "source": [
    "MBF_imputeScore=np.array([])\n",
    "\n",
    "for i in range(10):\n",
    "    MBF_missing=missingByFeature(X_hr_big_train,p1,p2)\n",
    "    MBF_imputed=imputetMissing(MBF_missing)\n",
    "    \n",
    "    #doing the predective performace thing\n",
    "    bestModel.fit(MBF_imputed,y_hr_big_train)\n",
    "    MBF_imputeScore=np.append(MBF_imputeScore,bestModel.score(X_hr_test,y_hr_test))\n",
    "    \n",
    "    #do the feature selection thing\n",
    "    imputed_feature_MBF=importantFeatures(bestModel,MBF_imputed,y_hr_big_train,BS_size_big)\n",
    "    imputed_MBF_Index,imputed_MBF_occurrence=featureImportance(imputed_feature_MBF)\n",
    "    \n",
    "    print('There are',len(mostCommonFeatures(imputed_MBF_occurrence)),'important features')\n",
    "    print(mostCommonFeatures(imputed_occurrence))\n",
    "print(MBF_imputeScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
