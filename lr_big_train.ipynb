{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "import random\n",
    "from sklearn.linear_model import Ridge\n",
    "from fancyimpute import SoftImpute, BiScaler\n",
    "from sklearn.linear_model import LinearRegression as reg\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "import itertools\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "import os, sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data=np.load('august2020-exercise1.npz')\n",
    "\n",
    "#print(data.files) # view the files in the npz\n",
    "\n",
    "#function to normalize the data\n",
    "def normalize(matrix):\n",
    "    normed=(matrix-matrix.mean(axis=0))/matrix.std(axis=0)\n",
    "    return normed\n",
    "\n",
    "######## train data ########\n",
    "X_lr_small_train=normalize(data['X_lr_small_train'])\n",
    "X_lr_big_train=normalize(data['X_lr_big_train'])\n",
    "X_hr_small_train=normalize(data['X_hr_small_train'])\n",
    "X_hr_big_train=normalize(data['X_hr_big_train'])\n",
    "\n",
    "y_lr_small_train=normalize(data['y_lr_small_train'])\n",
    "y_lr_big_train=normalize(data['y_lr_big_train'])\n",
    "y_hr_small_train=normalize(data['y_hr_small_train'])\n",
    "y_hr_big_train=normalize(data['y_hr_big_train'])\n",
    "######## test data#########\n",
    "X_lr_test=normalize(data['X_lr_test'])\n",
    "X_hr_test=normalize(data['X_hr_test'])\n",
    "\n",
    "y_lr_test=normalize(data['y_lr_test'])\n",
    "y_hr_test=normalize(data['y_hr_test'])\n",
    "\n",
    "def compRandomFunc(array,probability): # probability=0.7\n",
    "    n=array.shape[0]\n",
    "    p=array.shape[1]\n",
    "    array=array.astype(\"float\")\n",
    "    for i in range(n):    \n",
    "        for j in range(p):\n",
    "            r=random.uniform(0, 1)\n",
    "            if r < probability:\n",
    "                array[i,j]=np.NaN\n",
    "    return array\n",
    "\n",
    "def missingByFeature(array,p1,p2): #p1=0.875, p2=0.8\n",
    "    n=array.shape[0]\n",
    "    p=array.shape[1]\n",
    "    array=array.astype(\"float\")\n",
    "    featureList=np.array([])\n",
    "    for i in range(p):\n",
    "        r = random.uniform(0, 1)\n",
    "        if r<p1:\n",
    "            featureList=np.append(featureList,i)\n",
    "    for i in featureList:  # i is the counter for feature not sample size\n",
    "        i=int(i)\n",
    "        for j in range(n):\n",
    "            r2=random.uniform(0,1)\n",
    "            if r2<p2:\n",
    "                array[j,i]=np.NaN\n",
    "    return array\n",
    "\n",
    "def meanImputation(feature,data):\n",
    "    imputed=np.nanmean(data[:,feature])\n",
    "    return imputed\n",
    "\n",
    "# creating data with missing values\n",
    "import math\n",
    "probability=0.7\n",
    "p1=0.875\n",
    "p2=0.8\n",
    "# return the imputed matrix with mean imputation\n",
    "def imputetMissing(matrix):\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            if math.isnan(matrix[i,j]) is True:\n",
    "                matrix[i,j]=meanImputation(j,matrix)\n",
    "    return matrix\n",
    "\n",
    "def test(models, X_train,y_train,X_test,y_test, iterations = 100):\n",
    "    results = {}\n",
    "    for i in models:\n",
    "        r2_train = []\n",
    "        r2_test = []\n",
    "        for j in range(iterations):\n",
    "            r2_test.append(metrics.r2_score(y_test,\n",
    "                                            models[i].fit(X_train, \n",
    "                                                         y_train).predict(X_test)))\n",
    "\n",
    "            r2_train.append(metrics.r2_score(y_train, \n",
    "                                             models[i].fit(X_train, \n",
    "                                                          y_train).predict(X_train)))\n",
    "        results[i] = [np.mean(r2_train),np.mean(r2_test)]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def importantFeatures(best_model,X_train,y_train,BS_size):\n",
    "    importantFeatures=np.array([])\n",
    "    \n",
    "    for i in range(100):\n",
    "        #creatig bootstrap boots\n",
    "        x_bag=np.empty((0,X_train.shape[1]))\n",
    "        y_bag=np.array([])\n",
    "        for j in range(BS_size): \n",
    "            randIndex=np.random.randint(len(X_train), size=1)\n",
    "            x_sample=X_train[randIndex,:]\n",
    "            y_sample=y_train[randIndex]\n",
    "                \n",
    "            x_bag=np.concatenate((x_bag,x_sample))\n",
    "            y_bag=np.append(y_bag,y_sample)             \n",
    "        # test the estimator on bootstrap sample 100 times\n",
    "        #skippa grid search och kör dirr på bästa hyperparametrarna för att spara tid\n",
    "        bestEstimator=best_model.fit(x_bag,y_bag)\n",
    "        \n",
    "        rfeBest=RFE(bestEstimator)\n",
    "        X_rfe=rfeBest.fit_transform(x_bag,y_bag)\n",
    "        bestEstimator.fit(X_rfe,y_bag)\n",
    "        \n",
    "        importantFeatures=np.append(importantFeatures,np.where(rfeBest.ranking_==1))\n",
    "        \n",
    "    return importantFeatures\n",
    "\n",
    "def featureImportance(featureList):\n",
    "    unique_elements, counts_elements = np.unique(featureList, return_counts=True)\n",
    "    return unique_elements, counts_elements\n",
    "\n",
    "def mostCommonFeatures(occurrence):\n",
    "    mostCommonIndex=np.array([])\n",
    "    for i in range(len(occurrence)):\n",
    "        if occurrence[i]>=80:\n",
    "            mostCommonIndex=np.append(mostCommonIndex,[i])\n",
    "    mostCommonIndex=np.array(mostCommonIndex,dtype=np.int8)\n",
    "    mostCommonFeatures=featureIndex[mostCommonIndex]\n",
    "    return mostCommonFeatures\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low rate big data\n",
      "ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0,\n",
      "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "models = {'OLS': linear_model.LinearRegression(),\n",
    "         'Lasso': linear_model.Lasso(),\n",
    "         'Ridge': linear_model.Ridge(),}\n",
    "\n",
    "lasso_params = {'alpha':[0.02, 0.024, 0.025, 0.026, 0.03]}\n",
    "ridge_params = {'alpha':[200, 250, 300, 400, 500]}\n",
    "eNet_params={'alpha': [0, 0.5, 0.1, 0.01, 0.001],\n",
    "             'l1_ratio': [0, 0.25, 0.5, 0.75, 1]}\n",
    "\n",
    "models2 = {'OLS': linear_model.LinearRegression(),\n",
    "           'Lasso': GridSearchCV(linear_model.Lasso(), \n",
    "                               param_grid=lasso_params).fit(X_lr_big_train,y_lr_big_train).best_estimator_,\n",
    "           'Ridge': GridSearchCV(linear_model.Ridge(), \n",
    "                               param_grid=ridge_params).fit(X_lr_big_train,y_lr_big_train).best_estimator_,\n",
    "          'Elastic net':GridSearchCV(linear_model.ElasticNet(), \n",
    "                               param_grid=eNet_params).fit(X_lr_big_train,y_lr_big_train).best_estimator_}\n",
    "\n",
    "print('low rate big data')\n",
    "test(models2, X_lr_big_train,y_lr_big_train,X_lr_test,y_lr_test)\n",
    "\n",
    "print(GridSearchCV(linear_model.ElasticNet(),param_grid=eNet_params).fit(X_lr_big_train,y_lr_big_train).best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model score: 0.3187339776698195\n"
     ]
    }
   ],
   "source": [
    "bestModel=ElasticNet(l1_ratio=0,alpha=0.1)\n",
    "bestModel.fit(X_lr_big_train,y_lr_big_train)\n",
    "print('Best model score:',bestModel.score(X_lr_test,y_lr_test))\n",
    "#print('Best model coefficients:',bestModel.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS_size_big=1000\n",
    "feature_lr_big=importantFeatures(bestModel,X_lr_big_train,y_lr_big_train,BS_size_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13.,  21.,  36.,  63.,  73.,  81.,  87.,  98., 113.,  73.,  81.,\n",
       "        88., 104., 111., 115., 116., 143.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostCommonFeatures(occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureIndex,occurrence=featureImportance(feature_lr_big)\n",
    "len(mostCommonFeatures(occurrence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17 important features\n",
      "[ 54.  89. 101. 108. 112. 115. 119. 131. 151. 153. 155.  40.  52.  67.\n",
      "  81.  90. 112.]\n",
      "There are 21 important features\n",
      "[ 26.  54.  87.  89. 101. 112. 113. 115. 131. 151. 168.  62.  67.  73.\n",
      "  81.  90.  96.  97. 112. 113. 122.]\n",
      "There are 15 important features\n",
      "[ 89. 101. 112. 115. 124. 151. 155. 167.  52.  67.  73.  81.  90.  97.\n",
      " 113.]\n",
      "There are 18 important features\n",
      "[ 26.  54.  87.  89. 101. 112. 115. 119. 124. 161.  40.  52.  62.  81.\n",
      "  96.  97. 122. 131.]\n",
      "There are 15 important features\n",
      "[ 89. 101. 112. 113. 119. 124. 151.  62.  67.  79.  81.  90.  97.  99.\n",
      " 113.]\n",
      "There are 18 important features\n",
      "[ 26.  52.  57.  87.  89. 101. 112. 115. 132. 151. 155.  52.  67.  81.\n",
      "  90.  97. 104. 112.]\n",
      "There are 20 important features\n",
      "[ 26.  57.  87. 101. 112. 115. 119. 132. 151. 153.  40.  52.  62.  67.\n",
      "  69.  81.  90.  97. 122. 131.]\n",
      "There are 20 important features\n",
      "[ 54.  89. 101. 112. 119. 124. 133. 136. 151. 155. 161.  40.  52.  62.\n",
      "  67.  73.  81.  90.  97. 113.]\n",
      "There are 18 important features\n",
      "[ 26.  87.  89. 112. 115. 132. 151.  40.  47.  52.  62.  65.  67.  73.\n",
      "  81.  96.  97. 131.]\n",
      "There are 18 important features\n",
      "[ 24.  87. 101. 112. 113. 115. 119. 126. 151.  52.  62.  67.  90.  96.\n",
      "  97. 104. 113. 131.]\n",
      "[0.31609544 0.31536497 0.31670389 0.31671139 0.31600721 0.31639004\n",
      " 0.31739284 0.31630888 0.31695279 0.31571944]\n"
     ]
    }
   ],
   "source": [
    "#random missing and softImpute\n",
    "softImpute = SoftImpute()\n",
    "biscaler = BiScaler()\n",
    "softImputeScore=np.array([])\n",
    "scale=np.array([0.1,0.25,0.5,0.75])\n",
    "\n",
    "for i in range(10):\n",
    "    with HiddenPrints():\n",
    "        X_randomMissingSoft=compRandomFunc(X_lr_big_train,probability) #here is the line where you enable missing data\n",
    "        X_randomMissingSoft_normalized = biscaler.fit_transform(X_randomMissingSoft)\n",
    "        \n",
    "        lambda_0=np.nanmax(X_randomMissingSoft_normalized)\n",
    "        bestOfLambdaScore=np.array([])\n",
    "        for j in range(len(scale)):\n",
    "            softImpute=SoftImpute(shrinkage_value=lambda_0*scale[j])\n",
    "            \n",
    "            X_imputed_soft_normalized = softImpute.fit_transform(X_randomMissingSoft_normalized)\n",
    "            X_imputed_soft = biscaler.inverse_transform(X_imputed_soft_normalized)\n",
    "            \n",
    "            bestModel.fit(X_imputed_soft,y_lr_big_train)\n",
    "            bestOfLambdaScore=np.append(bestOfLambdaScore,bestModel.score(X_lr_test,y_lr_test))\n",
    "            \n",
    "        bestLambdaIndex=np.where(bestOfLambdaScore==np.max(bestOfLambdaScore))\n",
    "        softImpute=SoftImpute(shrinkage_value=lambda_0*scale[bestLambdaIndex])\n",
    "        X_imputed_soft_normalized = softImpute.fit_transform(X_randomMissingSoft_normalized)\n",
    "        X_imputed_soft = biscaler.inverse_transform(X_imputed_soft_normalized)\n",
    "        \n",
    "        softImputeScore=np.append(softImputeScore,np.max(bestOfLambdaScore))\n",
    "        \n",
    "    #do the feature selection thing\n",
    "    softImpute_feature=importantFeatures(bestModel,X_imputed_soft,y_lr_big_train,BS_size_big)\n",
    "    softImputed_featureIndex,softImputed_occurrence=featureImportance(softImpute_feature)\n",
    "    print('There are',len(mostCommonFeatures(softImputed_occurrence)),'important features',flush=True)\n",
    "    print(mostCommonFeatures(softImputed_occurrence),flush=True)\n",
    "print(softImputeScore,flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 important features\n",
      "[ 26.  54.  87. 112. 115. 153. 167.  40.  62.  67.  79.  90. 104.]\n",
      "There are 10 important features\n",
      "[ 54. 113. 119. 124.  47.  62.  67.  79.  81.  90.]\n",
      "There are 12 important features\n",
      "[ 54.  57.  89. 101. 112. 115. 155. 169.  62.  96. 113. 122.]\n",
      "There are 13 important features\n",
      "[101. 112. 115. 119. 124. 131. 151. 153.  40.  67.  73.  81. 112.]\n",
      "There are 12 important features\n",
      "[ 34. 101. 131. 155.  40.  54.  62.  67.  81.  96.  97. 131.]\n",
      "There are 11 important features\n",
      "[ 54. 113. 124. 136. 151.  52.  62.  67.  81.  86.  90.]\n",
      "There are 15 important features\n",
      "[ 54.  57.  89.  98. 101. 112. 119. 132. 151.  52.  81.  90.  97. 113.\n",
      " 131.]\n",
      "There are 14 important features\n",
      "[ 26.  52.  54.  89. 101. 119. 124. 148.  79.  81.  90.  97. 112. 122.]\n",
      "There are 12 important features\n",
      "[ 52.  54.  57. 101. 108. 112. 119. 151.  62.  67.  96.  97.]\n",
      "There are 9 important features\n",
      "[ 26.  87. 119. 155.  54.  67.  97. 113. 129.]\n",
      "[0.31185328 0.3124782  0.31131603 0.3152979  0.30892148 0.31011174\n",
      " 0.31495455 0.31409761 0.31068339 0.31334926]\n"
     ]
    }
   ],
   "source": [
    "#softImpute with missing by feature\n",
    "softImputeMBFScore=np.array([])\n",
    "for i in range(10):\n",
    "    with HiddenPrints():\n",
    "        X_MBF_soft=missingByFeature(X_lr_big_train,p1,p2)\n",
    "        X_MBF_soft_normalized=biscaler.fit_transform(X_MBF_soft)\n",
    "        \n",
    "        lambda_0=np.nanmax(X_randomMissingSoft_normalized)\n",
    "        bestOfLambdaScore=np.array([])\n",
    "        for j in range(len(scale)):\n",
    "            softImpute=SoftImpute(shrinkage_value=lambda_0*scale[j])\n",
    "            \n",
    "            X_MBF_imputed_soft_normalized=softImpute.fit_transform(X_MBF_soft_normalized)\n",
    "            X_MBF_imputed_soft=biscaler.inverse_transform(X_MBF_imputed_soft_normalized)\n",
    "            \n",
    "            bestModel.fit(X_MBF_imputed_soft,y_lr_big_train)\n",
    "            bestOfLambdaScore=np.append(bestOfLambdaScore,bestModel.score(X_lr_test,y_lr_test))\n",
    "        \n",
    "        bestLambdaIndex=np.where(bestOfLambdaScore==np.max(bestOfLambdaScore))\n",
    "        softImpute=SoftImpute(shrinkage_value=lambda_0*scale[bestLambdaIndex])\n",
    "        \n",
    "        X_MBF_imputed_soft_normalized=softImpute.fit_transform(X_MBF_soft_normalized)\n",
    "        X_MBF_imputed_soft=biscaler.inverse_transform(X_MBF_imputed_soft_normalized)\n",
    "        \n",
    "        softImputeMBFScore=np.append(softImputeMBFScore,np.max(bestOfLambdaScore))\n",
    "    \n",
    "    #do the feature selection thing\n",
    "    softImpute_MBF_feature=importantFeatures(bestModel,X_MBF_imputed_soft,y_lr_big_train,BS_size_big)\n",
    "    softImputed_MBF_featureIndex,softImputed_MBF_occurrence=featureImportance(softImpute_MBF_feature)\n",
    "    \n",
    "    print('There are',len(mostCommonFeatures(softImputed_MBF_occurrence)),'important features')\n",
    "    print(mostCommonFeatures(softImputed_MBF_occurrence))\n",
    "print(softImputeMBFScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 important features\n",
      "[ 24.  59. 101. 136.  54.  57.  62.  67.  90.  97. 100. 131.]\n",
      "There are 10 important features\n",
      "[ 47.  52. 124. 126. 136.  40.  67.  90. 113. 122.]\n",
      "There are 6 important features\n",
      "[111. 112.  40.  62.  67. 113.]\n",
      "There are 11 important features\n",
      "[ 57.  91. 112. 119. 129. 136.  40.  57.  62.  90. 113.]\n",
      "There are 12 important features\n",
      "[ 26.  30.  52.  54.  57.  59.  89. 151.  67.  81.  97. 131.]\n",
      "There are 6 important features\n",
      "[126. 136. 143. 155.  40. 112.]\n",
      "There are 9 important features\n",
      "[ 72. 101. 136.  52.  54.  67.  90.  97. 112.]\n",
      "There are 11 important features\n",
      "[ 26.  52.  91. 115. 124. 151.  54.  81.  86. 114. 129.]\n",
      "There are 11 important features\n",
      "[ 40.  72.  91. 124. 161.  47.  65.  67.  81.  90. 131.]\n",
      "There are 10 important features\n",
      "[101. 112. 136. 151.  47.  52.  65.  67.  90.  97.]\n",
      "[-0.92455693 -0.94097812 -0.98028143 -0.93147721 -0.92866446 -0.93930999\n",
      " -0.87320616 -0.91487438 -0.90202767 -0.84803664]\n"
     ]
    }
   ],
   "source": [
    "# random missing and mean impute\n",
    "imputeScore=np.array([])\n",
    "for i in range(10): \n",
    "    \n",
    "    missing_random=compRandomFunc(X_lr_big_train,probability)\n",
    "    imputed_random=imputetMissing(missing_random)\n",
    "    \n",
    "    #doing the predective performance thing\n",
    "    bestModel.fit(imputed_random,y_lr_big_train)\n",
    "    imputeScore=np.append(imputeScore,bestModel.score(X_lr_test,y_lr_test))\n",
    "    \n",
    "    #do the feature selection thing\n",
    "    imputed_feature=importantFeatures(bestModel,imputed_random,y_lr_big_train,BS_size_big)\n",
    "    imputed_featureIndex,imputed_occurrence=featureImportance(imputed_feature)\n",
    "    print('There are',len(mostCommonFeatures(imputed_occurrence)),'important features')\n",
    "    print(mostCommonFeatures(imputed_occurrence))\n",
    "    \n",
    "print(imputeScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 important features\n",
      "[101. 112. 136. 151.  47.  52.  65.  67.  90.  97.]\n",
      "There are 11 important features\n",
      "[101. 112. 136. 151.  47.  52.  65.  67.  90.  97.]\n",
      "There are 12 important features\n",
      "[101. 112. 136. 151.  47.  52.  65.  67.  90.  97.]\n",
      "There are 15 important features\n",
      "[101. 112. 136. 151.  47.  52.  65.  67.  90.  97.]\n",
      "There are 10 important features\n",
      "[101. 112. 136. 151.  47.  52.  65.  67.  90.  97.]\n",
      "There are 14 important features\n",
      "[101. 112. 136. 151.  47.  52.  65.  67.  90.  97.]\n",
      "There are 10 important features\n",
      "[101. 112. 136. 151.  47.  52.  65.  67.  90.  97.]\n",
      "There are 11 important features\n",
      "[101. 112. 136. 151.  47.  52.  65.  67.  90.  97.]\n",
      "There are 12 important features\n",
      "[101. 112. 136. 151.  47.  52.  65.  67.  90.  97.]\n",
      "There are 12 important features\n",
      "[101. 112. 136. 151.  47.  52.  65.  67.  90.  97.]\n",
      "[ 0.17968581  0.08227616  0.09648213  0.16452436  0.29294119  0.09028483\n",
      "  0.22162916  0.17156588 -0.01895995  0.26526203]\n"
     ]
    }
   ],
   "source": [
    "MBF_imputeScore=np.array([])\n",
    "\n",
    "for i in range(10):\n",
    "    MBF_missing=missingByFeature(X_lr_big_train,p1,p2)\n",
    "    MBF_imputed=imputetMissing(MBF_missing)\n",
    "    \n",
    "    #doing the predective performace thing\n",
    "    bestModel.fit(MBF_imputed,y_lr_big_train)\n",
    "    MBF_imputeScore=np.append(MBF_imputeScore,bestModel.score(X_lr_test,y_lr_test))\n",
    "    \n",
    "    #do the feature selection thing\n",
    "    imputed_feature_MBF=importantFeatures(bestModel,MBF_imputed,y_lr_big_train,BS_size_big)\n",
    "    imputed_MBF_Index,imputed_MBF_occurrence=featureImportance(imputed_feature_MBF)\n",
    "    \n",
    "    print('There are',len(mostCommonFeatures(imputed_MBF_occurrence)),'important features')\n",
    "    print(mostCommonFeatures(imputed_occurrence))\n",
    "print(MBF_imputeScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
